{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Multi-Map Training on Google Colab (L4 GPU Optimized)\n",
    "\n",
    "This notebook trains a **generalized policy** that works across **all maps** using parallel environment execution.\n",
    "\n",
    "## Key Features:\n",
    "- **Parallel Training**: Multiple environments run simultaneously\n",
    "- **Multi-Map Learning**: Trains on all maps at once for generalization\n",
    "- **Curriculum Learning**: Starts easy, gradually increases difficulty\n",
    "- **L4 GPU Optimized**: Batched operations for maximum GPU utilization\n",
    "- **Easy to Extend**: Simple to add new maps\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Enable L4 GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU ‚Üí L4\n",
    "2. **Run all cells in order**\n",
    "3. **Training saves checkpoints** automatically\n",
    "4. **Download checkpoints** when complete\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n‚úÖ GPU DETECTED\")\n",
    "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
    "    \n",
    "    # Check if L4\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'L4' in gpu_name:\n",
    "        print(\"\\nüöÄ L4 GPU DETECTED - Optimal configuration!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  GPU is {gpu_name}, not L4\")\n",
    "        print(\"For best performance, select L4 GPU in Runtime settings\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED\")\n",
    "    print(\"Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Ben-jpg-del/CalHacks.git\n",
    "%cd CalHacks\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch numpy pygame wandb -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Environment and Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_multi_map_env import MapRegistry, ParallelMultiMapEnv\n",
    "import torch\n",
    "\n",
    "# Check available maps\n",
    "print(\"Available Maps:\")\n",
    "print(\"=\" * 60)\n",
    "for map_name in MapRegistry.get_map_names():\n",
    "    level = MapRegistry.get_map(map_name)\n",
    "    print(f\"  {map_name.upper()}\")\n",
    "    print(f\"    - Dimensions: {level.width}x{level.height}\")\n",
    "    print(f\"    - Platforms: {len(level.base_solids)}\")\n",
    "    print(f\"    - Hazards: {list(level.get_hazards().keys())}\")\n",
    "    print()\n",
    "\n",
    "# Test parallel environment\n",
    "print(\"Testing Parallel Environment:\")\n",
    "print(\"=\" * 60)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "env = ParallelMultiMapEnv(num_envs_per_map=4, device=device)\n",
    "print(env.get_statistics())\n",
    "\n",
    "# Test reset and step\n",
    "fire_obs, water_obs = env.reset()\n",
    "print(f\"\\nObservation shape: {fire_obs.shape}\")\n",
    "print(f\"Observation device: {fire_obs.device}\")\n",
    "print(\"\\n‚úÖ Environment test passed!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training\n",
    "\n",
    "**Adjust these parameters as needed:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Training settings\n",
    "NUM_EPISODES = 5000          # Total episodes to train\n",
    "NUM_ENVS_PER_MAP = 8         # Parallel environments per map (L4 can handle 8-16)\n",
    "USE_CURRICULUM = True        # Start easy, gradually add harder maps\n",
    "REWARD_TYPE = 'dense'        # Options: 'sparse', 'dense', 'cooperation', 'safety'\n",
    "\n",
    "# Curriculum schedule (episode -> map distribution)\n",
    "# Customize this to control learning difficulty progression\n",
    "CURRICULUM_SCHEDULE = {\n",
    "    0: {'tutorial': 1.0},                          # Start: 100% tutorial\n",
    "    500: {'tutorial': 0.7, 'tower': 0.3},         # Episode 500: Add 30% tower\n",
    "    1000: {'tutorial': 0.5, 'tower': 0.5},        # Episode 1000: Equal split\n",
    "    2000: {'tutorial': 0.3, 'tower': 0.7},        # Episode 2000: Focus on tower\n",
    "    # Add new maps here:\n",
    "    # 3000: {'tutorial': 0.2, 'tower': 0.3, 'new_map': 0.5},\n",
    "}\n",
    "\n",
    "# Alternative: Fixed distribution (if USE_CURRICULUM = False)\n",
    "MAP_DISTRIBUTION = {\n",
    "    'tutorial': 0.5,\n",
    "    'tower': 0.5,\n",
    "}\n",
    "\n",
    "# Checkpoint settings\n",
    "SAVE_DIR = 'checkpoints_multimap'\n",
    "SAVE_FREQ = 100              # Save every N episodes\n",
    "LOG_FREQ = 10                # Log every N episodes\n",
    "\n",
    "# Resume training (optional)\n",
    "RESUME_FROM = None           # e.g., 'checkpoints_multimap/checkpoint_ep1000'\n",
    "\n",
    "# Weights & Biases (optional)\n",
    "USE_WANDB = False\n",
    "WANDB_PROJECT = 'firewater-multimap'\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Environments per Map: {NUM_ENVS_PER_MAP}\")\n",
    "print(f\"Curriculum Learning: {USE_CURRICULUM}\")\n",
    "print(f\"Reward Type: {REWARD_TYPE}\")\n",
    "print(f\"Save Directory: {SAVE_DIR}\")\n",
    "if RESUME_FROM:\n",
    "    print(f\"\\n‚ö†Ô∏è  RESUMING from {RESUME_FROM}\")\n",
    "else:\n",
    "    print(f\"\\nStarting fresh training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Estimate parallel environments\n",
    "num_maps = len(MapRegistry.get_map_names())\n",
    "total_envs = NUM_ENVS_PER_MAP * num_maps\n",
    "print(f\"\\nEstimated parallel environments: ~{total_envs}\")\n",
    "print(f\"GPU utilization: {'High ‚úÖ' if total_envs >= 8 else 'Medium ‚ö†Ô∏è'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Existing Checkpoints (OPTIONAL - For Resuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "print(\"üì§ Upload checkpoints.zip to resume training\")\n",
    "print(\"(Skip this cell if starting fresh)\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'checkpoints.zip' in uploaded or 'checkpoints_multimap.zip' in uploaded:\n",
    "    zip_name = 'checkpoints.zip' if 'checkpoints.zip' in uploaded else 'checkpoints_multimap.zip'\n",
    "    \n",
    "    # Extract\n",
    "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {zip_name}\")\n",
    "    \n",
    "    # List checkpoints\n",
    "    if os.path.exists(SAVE_DIR):\n",
    "        checkpoints = [d for d in os.listdir(SAVE_DIR) if d.startswith('checkpoint_ep')]\n",
    "        if checkpoints:\n",
    "            print(f\"\\nFound {len(checkpoints)} checkpoint(s)\")\n",
    "            for cp in sorted(checkpoints)[-5:]:\n",
    "                print(f\"  - {cp}\")\n",
    "            \n",
    "            # Suggest latest\n",
    "            latest = sorted(checkpoints)[-1]\n",
    "            print(f\"\\nüí° Latest checkpoint: {latest}\")\n",
    "            print(f\"üí° Set RESUME_FROM = '{SAVE_DIR}/{latest}' in config cell\")\n",
    "else:\n",
    "    print(\"No checkpoints uploaded. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Weights & Biases (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    print(\"‚úÖ Logged in to W&B\")\n",
    "else:\n",
    "    print(\"W&B disabled. Set USE_WANDB=True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Agents\n",
    "\n",
    "This will train a **generalized policy** across all maps using parallel environments.\n",
    "\n",
    "**Training Features:**\n",
    "- Parallel environment execution\n",
    "- Multi-map simultaneous training\n",
    "- Curriculum learning (optional)\n",
    "- Automatic checkpointing\n",
    "- Per-map success tracking\n",
    "\n",
    "**Note**: Training may take several hours depending on NUM_EPISODES and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reload module to get latest changes (in case it was imported before)\nimport importlib\nimport sys\nif 'train_parallel_multimap' in sys.modules:\n    import train_parallel_multimap\n    importlib.reload(train_parallel_multimap)\n    \nfrom train_parallel_multimap import train_parallel_multimap\n\nprint(\"üöÄ Starting Parallel Multi-Map Training...\")\nprint(\"=\" * 80)\n\ntrain_parallel_multimap(\n    num_episodes=NUM_EPISODES,\n    num_envs_per_map=NUM_ENVS_PER_MAP,\n    map_distribution=MAP_DISTRIBUTION if not USE_CURRICULUM else None,\n    use_curriculum=USE_CURRICULUM,\n    curriculum_schedule=CURRICULUM_SCHEDULE if USE_CURRICULUM else None,\n    reward_type=REWARD_TYPE,\n    save_dir=SAVE_DIR,\n    save_freq=SAVE_FREQ,\n    log_freq=LOG_FREQ,\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    resume_from=RESUME_FROM,\n    use_wandb=USE_WANDB,\n    wandb_project=WANDB_PROJECT\n)\n\nprint(\"\\n‚úÖ Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. List Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    checkpoints = sorted([d for d in os.listdir(SAVE_DIR) if d.startswith('checkpoint_')])\n",
    "    \n",
    "    print(f\"Checkpoints in {SAVE_DIR}:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for cp in checkpoints:\n",
    "        cp_dir = os.path.join(SAVE_DIR, cp)\n",
    "        if os.path.isdir(cp_dir):\n",
    "            files = os.listdir(cp_dir)\n",
    "            fire_exists = 'fire_agent.pth' in files\n",
    "            water_exists = 'water_agent.pth' in files\n",
    "            status = \"‚úÖ\" if (fire_exists and water_exists) else \"‚ö†Ô∏è\"\n",
    "            print(f\"{status} {cp}\")\n",
    "    \n",
    "    # Check final\n",
    "    final_dir = os.path.join(SAVE_DIR, 'final')\n",
    "    if os.path.exists(final_dir):\n",
    "        print(f\"\\n‚úÖ Final checkpoint saved\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Total checkpoints: {len(checkpoints)}\")\n",
    "else:\n",
    "    print(f\"No checkpoints found in {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Trained Agent on All Maps\n",
    "\n",
    "Test how well the generalized policy works on each map individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_parallel_multimap import ParallelDQNAgent\n",
    "from parallel_multi_map_env import MapRegistry\n",
    "from game_environment import FireWaterEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Select checkpoint to evaluate\n",
    "EVAL_CHECKPOINT = 'final'  # or 'checkpoint_ep1000', etc.\n",
    "NUM_EVAL_EPISODES = 100    # Episodes per map\n",
    "\n",
    "print(f\"Evaluating checkpoint: {EVAL_CHECKPOINT}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load agents\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "fire_agent = ParallelDQNAgent(device=device)\n",
    "water_agent = ParallelDQNAgent(device=device)\n",
    "\n",
    "checkpoint_dir = os.path.join(SAVE_DIR, EVAL_CHECKPOINT)\n",
    "fire_agent.load(os.path.join(checkpoint_dir, 'fire_agent.pth'))\n",
    "water_agent.load(os.path.join(checkpoint_dir, 'water_agent.pth'))\n",
    "\n",
    "# Set to evaluation mode\n",
    "fire_agent.epsilon = 0.0\n",
    "water_agent.epsilon = 0.0\n",
    "\n",
    "print(f\"‚úÖ Agents loaded from {checkpoint_dir}\\n\")\n",
    "\n",
    "# Evaluate on each map\n",
    "all_results = {}\n",
    "\n",
    "for map_name in MapRegistry.get_map_names():\n",
    "    print(f\"Testing on {map_name.upper()} map...\")\n",
    "    \n",
    "    # Create environment for this map\n",
    "    level = MapRegistry.get_map(map_name)\n",
    "    env = FireWaterEnv(level=level)\n",
    "    \n",
    "    successes = 0\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(NUM_EVAL_EPISODES):\n",
    "        fire_obs, water_obs = env.reset()\n",
    "        fire_obs = torch.FloatTensor(fire_obs).unsqueeze(0).to(device)\n",
    "        water_obs = torch.FloatTensor(water_obs).unsqueeze(0).to(device)\n",
    "        \n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 3000:\n",
    "            # Get actions\n",
    "            fire_action = fire_agent.select_actions(fire_obs, training=False).item()\n",
    "            water_action = water_agent.select_actions(water_obs, training=False).item()\n",
    "            \n",
    "            # Step\n",
    "            (fire_obs_np, water_obs_np), (fire_reward, water_reward), \\\n",
    "            (fire_done, water_done), info = env.step(fire_action, water_action)\n",
    "            \n",
    "            fire_obs = torch.FloatTensor(fire_obs_np).unsqueeze(0).to(device)\n",
    "            water_obs = torch.FloatTensor(water_obs_np).unsqueeze(0).to(device)\n",
    "            \n",
    "            episode_reward += fire_reward + water_reward\n",
    "            steps += 1\n",
    "            done = fire_done or water_done\n",
    "        \n",
    "        if info.get('both_won', False):\n",
    "            successes += 1\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[map_name] = {\n",
    "        'success_rate': successes / NUM_EVAL_EPISODES * 100,\n",
    "        'avg_reward': np.mean(total_rewards),\n",
    "        'avg_length': np.mean(episode_lengths),\n",
    "        'successes': successes\n",
    "    }\n",
    "    \n",
    "    print(f\"  Success Rate: {all_results[map_name]['success_rate']:.1f}%\")\n",
    "    print(f\"  Avg Reward: {all_results[map_name]['avg_reward']:.2f}\")\n",
    "    print(f\"  Avg Length: {all_results[map_name]['avg_length']:.1f} steps\")\n",
    "    print()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Summary\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "overall_success = np.mean([r['success_rate'] for r in all_results.values()])\n",
    "print(f\"Overall Success Rate: {overall_success:.1f}%\")\n",
    "print(f\"\\nPer-Map Results:\")\n",
    "for map_name, results in all_results.items():\n",
    "    print(f\"  {map_name.upper()}: {results['success_rate']:.1f}% ({results['successes']}/{NUM_EVAL_EPISODES})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip all checkpoints\n",
    "shutil.make_archive('checkpoints_multimap', 'zip', SAVE_DIR)\n",
    "\n",
    "print(\"‚úÖ Checkpoints zipped!\")\n",
    "print(f\"\\nFile: checkpoints_multimap.zip\")\n",
    "print(f\"Size: {os.path.getsize('checkpoints_multimap.zip') / 1e6:.1f} MB\")\n",
    "print(\"\\nDownload from Files panel (left sidebar) ‚Üí‚Üí‚Üí\")\n",
    "\n",
    "# Alternative: Auto-download\n",
    "from google.colab import files\n",
    "# files.download('checkpoints_multimap.zip')  # Uncomment to auto-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Locally (Instructions)\n",
    "\n",
    "To visualize your trained generalized policy:\n",
    "\n",
    "1. **Download** `checkpoints_multimap.zip` from Colab\n",
    "2. **Extract** to your local repository\n",
    "3. **Run visualization** on any map:\n",
    "\n",
    "```bash\n",
    "# On your local machine:\n",
    "\n",
    "# Tutorial map\n",
    "python visualize.py trained checkpoints_multimap/final/fire_agent.pth checkpoints_multimap/final/water_agent.pth --map tutorial\n",
    "\n",
    "# Tower map\n",
    "python visualize.py trained checkpoints_multimap/final/fire_agent.pth checkpoints_multimap/final/water_agent.pth --map tower\n",
    "\n",
    "# Any new map you add\n",
    "python visualize.py trained checkpoints_multimap/final/fire_agent.pth checkpoints_multimap/final/water_agent.pth --map your_new_map\n",
    "```\n",
    "\n",
    "**Note**: The policy is trained on all maps, so it should generalize well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Adding New Maps (Instructions)\n",
    "\n",
    "To add a new map to the training:\n",
    "\n",
    "### Step 1: Create Map File\n",
    "Create `map_3.py` (or any name) with your level definition:\n",
    "\n",
    "```python\n",
    "# map_3.py\n",
    "from physics_engine import Rect\n",
    "\n",
    "class LevelConfig:\n",
    "    def __init__(self, level_name=\"My New Map\"):\n",
    "        self.name = level_name\n",
    "        self.width = 960\n",
    "        self.height = 540\n",
    "        # ... define platforms, hazards, etc.\n",
    "\n",
    "class LevelLibrary:\n",
    "    @staticmethod\n",
    "    def get_my_map():\n",
    "        return LevelConfig(\"My New Map\")\n",
    "```\n",
    "\n",
    "### Step 2: Register Map\n",
    "Edit `parallel_multi_map_env.py`, add to `MapRegistry.get_all_maps()`:\n",
    "\n",
    "```python\n",
    "from map_3 import LevelLibrary as Map3Library\n",
    "\n",
    "@staticmethod\n",
    "def get_all_maps():\n",
    "    return {\n",
    "        'tutorial': LevelLibrary.get_tutorial_level(),\n",
    "        'tower': Map1Library.get_tower_level(),\n",
    "        'my_new_map': Map3Library.get_my_map(),  # Add this line\n",
    "    }\n",
    "```\n",
    "\n",
    "### Step 3: Update Curriculum (Optional)\n",
    "In the config cell above, add to `CURRICULUM_SCHEDULE`:\n",
    "\n",
    "```python\n",
    "CURRICULUM_SCHEDULE = {\n",
    "    0: {'tutorial': 1.0},\n",
    "    500: {'tutorial': 0.7, 'tower': 0.3},\n",
    "    1000: {'tutorial': 0.5, 'tower': 0.5},\n",
    "    2000: {'tutorial': 0.3, 'tower': 0.4, 'my_new_map': 0.3},  # Add new map\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 4: Re-run Training\n",
    "The system will automatically include your new map!\n",
    "\n",
    "That's it! The parallel training system handles everything else automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}