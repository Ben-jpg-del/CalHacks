# GPU Training Quick Start

Get started with GPU-accelerated training in 5 minutes!

## âš¡ TL;DR

```bash
cd gpu_training
python test_gpu.py          # Verify everything works
jupyter notebook GPU_Training.ipynb  # Start training!
```

## ğŸ“‹ Prerequisites

1. **CUDA-compatible GPU** (NVIDIA)
2. **PyTorch with CUDA** installed
3. **8+ GB GPU RAM** recommended

Check GPU:
```python
import torch
print(torch.cuda.is_available())  # Should be True
print(torch.cuda.get_device_name(0))
```

## ğŸš€ Method 1: Jupyter Notebook (Recommended)

```bash
cd gpu_training
jupyter notebook GPU_Training.ipynb
```

Then:
1. Run all cells in order
2. Adjust `NUM_ENVS` based on your GPU memory
3. Wait for training to complete
4. Checkpoints saved to `checkpoints_gpu/`

**Recommended settings**:
- **8 GB GPU**: `NUM_ENVS = 512`
- **16 GB GPU**: `NUM_ENVS = 1024`
- **24+ GB GPU**: `NUM_ENVS = 2048`

## ğŸ–¥ï¸ Method 2: Command Line

```bash
cd gpu_training

# Basic training
python train_gpu.py

# With 2048 parallel environments
python train_gpu.py --envs 2048

# With wandb logging
python train_gpu.py --wandb --envs 1024

# Full custom training
python train_gpu.py \
    --envs 1024 \
    --episodes 5000 \
    --batch-size 1024 \
    --device cuda \
    --wandb \
    --save-dir my_checkpoints
```

## ğŸ“Š What to Expect

### Training Speed
- **CPU (original)**: ~200 steps/second (24 envs)
- **GPU (this)**: ~20,000 steps/second (1024 envs)
- **Speedup**: 100x faster!

### Training Time
- **5000 episodes** @ 1024 envs â‰ˆ **1-2 hours** (vs 100+ hours on CPU)
- Real-time progress logging every 10 episodes
- Auto-saves checkpoints every 100 episodes

### Memory Usage
```
1024 envs:
  - Observations: ~200 MB
  - Replay Buffer: ~4 GB
  - Networks: ~50 MB
  - Total: ~5 GB GPU RAM
```

## ğŸ® Visualize Results

After training:

```bash
# From main CalHacks directory
cd ..

# Visualize on tutorial map
python visualize.py trained \
    gpu_training/checkpoints_gpu/final/fire_agent.pth \
    gpu_training/checkpoints_gpu/final/water_agent.pth \
    --map tutorial

# Try tower map
python visualize.py trained \
    gpu_training/checkpoints_gpu/final/fire_agent.pth \
    gpu_training/checkpoints_gpu/final/water_agent.pth \
    --map tower

# Try map2
python visualize.py trained \
    gpu_training/checkpoints_gpu/final/fire_agent.pth \
    gpu_training/checkpoints_gpu/final/water_agent.pth \
    --map map2
```

## âœ… Verify Installation

Run tests before training:

```bash
cd gpu_training
python test_gpu.py
```

Should see:
```
============================================================
GPU Training System Tests
============================================================

âœ… GPU Available: NVIDIA GeForce RTX 3090
   Memory: 24.0 GB

Testing Physics Engine...
  âœ… Physics engine working

Testing GPU Environment...
  âœ… Environment created
  âœ… Reset working
  âœ… Stepping working
  âœ… Visualization export working

Testing GPU Agent...
  âœ… Forward pass working
  âœ… Action selection working

Testing Replay Buffer...
  âœ… Buffer push working
  âœ… Buffer sampling working

Testing Training Loop...
  âœ… Training loop working
  Steps: 50, Buffer: 400

============================================================
âœ… ALL TESTS PASSED!
============================================================
```

## ğŸ› Troubleshooting

### "CUDA out of memory"

**Solution**: Reduce number of environments

```python
# In notebook
NUM_ENVS = 512  # Instead of 1024

# Command line
python train_gpu.py --envs 512
```

### "No module named 'torch_env'"

**Solution**: Make sure you're in the `gpu_training` directory

```bash
cd gpu_training
python train_gpu.py
```

### Training is slow

**Check GPU usage**:
```bash
# In another terminal
watch -n 1 nvidia-smi
```

Should show:
- GPU-Util: ~90-100%
- Memory-Usage: High

If low, try increasing `NUM_ENVS` or `BATCH_SIZE`

### Import errors

**Make sure parent directory is accessible**:
```bash
# From gpu_training folder
cd ..
ls  # Should see map_config.py, map_1.py, map_2.py, etc.
cd gpu_training
```

## ğŸ“ˆ Monitoring Training

### Console Output
```
Episode 10/5000
  Avg Reward: 12.34
  Success Rate: 5.2%
  Epsilon: 0.950
  Steps/sec: 18743
  Buffer: 10240
  Time: 0.5m
```

### Weights & Biases

Enable with `USE_WANDB = True` in notebook or `--wandb` flag:

```bash
python train_gpu.py --wandb
```

Then view at: `https://wandb.ai/your-username/firewater-gpu`

## ğŸ’¾ Checkpoints

Checkpoints saved to:
```
checkpoints_gpu/
â”œâ”€â”€ checkpoint_ep100/
â”‚   â”œâ”€â”€ fire_agent.pth
â”‚   â””â”€â”€ water_agent.pth
â”œâ”€â”€ checkpoint_ep200/
â”‚   â”œâ”€â”€ fire_agent.pth
â”‚   â””â”€â”€ water_agent.pth
â”œâ”€â”€ ...
â””â”€â”€ final/
    â”œâ”€â”€ fire_agent.pth
    â””â”€â”€ water_agent.pth
```

Each checkpoint contains:
- Network weights
- Epsilon value
- Compatible with `visualize.py`

## ğŸ¯ Recommended Workflow

1. **Test first**:
   ```bash
   python test_gpu.py
   ```

2. **Short test run** (100 episodes):
   ```bash
   python train_gpu.py --episodes 100 --envs 512
   ```

3. **Check visualization**:
   ```bash
   cd ..
   python visualize.py trained gpu_training/checkpoints_gpu/final/fire_agent.pth gpu_training/checkpoints_gpu/final/water_agent.pth --map tutorial
   ```

4. **Full training** if looks good:
   ```bash
   cd gpu_training
   python train_gpu.py --episodes 5000 --envs 1024 --wandb
   ```

## ğŸ“š Next Steps

- Read `README.md` for detailed documentation
- Adjust hyperparameters in config section
- Try different map distributions
- Monitor with wandb for better insights

## ğŸ†˜ Getting Help

If you encounter issues:

1. Check GPU is detected: `nvidia-smi`
2. Run tests: `python test_gpu.py`
3. Try smaller batch: `--envs 256`
4. Check parent directory imports work
5. Verify PyTorch CUDA version matches GPU

---

**Happy GPU Training!** ğŸš€

You're about to train 100x faster than before!
